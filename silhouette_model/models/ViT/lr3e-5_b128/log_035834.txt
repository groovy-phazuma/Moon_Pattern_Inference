[20231203-035844] [INFO] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
[20231203-035844] [INFO] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
[20231203-040221] [INFO] Epoch: 1, train_loss: 6.8456, val_loss: 6.7428, train_acc: 0.0011, val_acc: 0.0057
[20231203-040221] [INFO] elapsed_time: 3.59 min
[20231203-040555] [INFO] Epoch: 2, train_loss: 6.5680, val_loss: 6.1421, train_acc: 0.0055, val_acc: 0.0120
[20231203-040555] [INFO] elapsed_time: 7.15 min
[20231203-040927] [INFO] Epoch: 3, train_loss: 5.7911, val_loss: 5.2685, train_acc: 0.0164, val_acc: 0.0323
[20231203-040927] [INFO] elapsed_time: 10.69 min
[20231203-041259] [INFO] Epoch: 4, train_loss: 5.0256, val_loss: 4.6596, train_acc: 0.0383, val_acc: 0.0560
[20231203-041259] [INFO] elapsed_time: 14.23 min
[20231203-041632] [INFO] Epoch: 5, train_loss: 4.4744, val_loss: 4.1948, train_acc: 0.0594, val_acc: 0.0918
[20231203-041632] [INFO] elapsed_time: 17.77 min
[20231203-042003] [INFO] Epoch: 6, train_loss: 4.0695, val_loss: 3.8814, train_acc: 0.0843, val_acc: 0.1154
[20231203-042003] [INFO] elapsed_time: 21.30 min
[20231203-042336] [INFO] Epoch: 7, train_loss: 3.7619, val_loss: 3.5814, train_acc: 0.1165, val_acc: 0.1439
[20231203-042336] [INFO] elapsed_time: 24.84 min
[20231203-042709] [INFO] Epoch: 8, train_loss: 3.5147, val_loss: 3.4203, train_acc: 0.1458, val_acc: 0.1615
[20231203-042709] [INFO] elapsed_time: 28.39 min
[20231203-043041] [INFO] Epoch: 9, train_loss: 3.2500, val_loss: 3.1539, train_acc: 0.1813, val_acc: 0.2167
[20231203-043041] [INFO] elapsed_time: 31.93 min
[20231203-043413] [INFO] Epoch: 10, train_loss: 3.1168, val_loss: 3.0114, train_acc: 0.2024, val_acc: 0.2372
[20231203-043413] [INFO] elapsed_time: 35.46 min
[20231203-043746] [INFO] Epoch: 11, train_loss: 2.9472, val_loss: 2.8838, train_acc: 0.2364, val_acc: 0.2520
[20231203-043746] [INFO] elapsed_time: 39.01 min
[20231203-044119] [INFO] Epoch: 12, train_loss: 2.7480, val_loss: 2.7628, train_acc: 0.2719, val_acc: 0.2795
[20231203-044119] [INFO] elapsed_time: 42.55 min
[20231203-044451] [INFO] Epoch: 13, train_loss: 2.6362, val_loss: 2.6383, train_acc: 0.2971, val_acc: 0.3128
[20231203-044451] [INFO] elapsed_time: 46.09 min
[20231203-044824] [INFO] Epoch: 14, train_loss: 2.4557, val_loss: 2.5883, train_acc: 0.3381, val_acc: 0.3342
[20231203-044824] [INFO] elapsed_time: 49.64 min
[20231203-045157] [INFO] Epoch: 15, train_loss: 2.2861, val_loss: 2.5403, train_acc: 0.3717, val_acc: 0.3257
[20231203-045157] [INFO] elapsed_time: 53.19 min
[20231203-045529] [INFO] Epoch: 16, train_loss: 2.2536, val_loss: 2.4474, train_acc: 0.3875, val_acc: 0.3562
[20231203-045529] [INFO] elapsed_time: 56.72 min
[20231203-045901] [INFO] Epoch: 17, train_loss: 2.0985, val_loss: 2.3767, train_acc: 0.4259, val_acc: 0.3734
[20231203-045901] [INFO] elapsed_time: 60.26 min
[20231203-050234] [INFO] Epoch: 18, train_loss: 2.0067, val_loss: 2.3412, train_acc: 0.4403, val_acc: 0.3825
[20231203-050234] [INFO] elapsed_time: 63.81 min
[20231203-050608] [INFO] Epoch: 19, train_loss: 1.9661, val_loss: 2.2641, train_acc: 0.4584, val_acc: 0.3945
[20231203-050608] [INFO] elapsed_time: 67.37 min
[20231203-050940] [INFO] Epoch: 20, train_loss: 1.8757, val_loss: 2.3155, train_acc: 0.4878, val_acc: 0.3944
[20231203-050940] [INFO] elapsed_time: 70.91 min
[20231203-051312] [INFO] Epoch: 21, train_loss: 1.8911, val_loss: 2.3584, train_acc: 0.4793, val_acc: 0.3726
[20231203-051312] [INFO] elapsed_time: 74.44 min
[20231203-051645] [INFO] Epoch: 22, train_loss: 1.7675, val_loss: 2.1241, train_acc: 0.5174, val_acc: 0.4422
[20231203-051645] [INFO] elapsed_time: 78.00 min
[20231203-052018] [INFO] Epoch: 23, train_loss: 1.6594, val_loss: 2.1240, train_acc: 0.5457, val_acc: 0.4380
[20231203-052018] [INFO] elapsed_time: 81.53 min
[20231203-052349] [INFO] Epoch: 24, train_loss: 1.6343, val_loss: 2.1091, train_acc: 0.5463, val_acc: 0.4437
[20231203-052349] [INFO] elapsed_time: 85.07 min
[20231203-052723] [INFO] Epoch: 25, train_loss: 1.6372, val_loss: 2.0787, train_acc: 0.5486, val_acc: 0.4579
[20231203-052723] [INFO] elapsed_time: 88.62 min
[20231203-053055] [INFO] Epoch: 26, train_loss: 1.5621, val_loss: 2.0187, train_acc: 0.5711, val_acc: 0.4761
[20231203-053055] [INFO] elapsed_time: 92.15 min
[20231203-053427] [INFO] Epoch: 27, train_loss: 1.5322, val_loss: 2.0074, train_acc: 0.5801, val_acc: 0.4655
[20231203-053427] [INFO] elapsed_time: 95.69 min
[20231203-053800] [INFO] Epoch: 28, train_loss: 1.4981, val_loss: 1.9779, train_acc: 0.6033, val_acc: 0.4774
[20231203-053800] [INFO] elapsed_time: 99.24 min
[20231203-054133] [INFO] Epoch: 29, train_loss: 1.4573, val_loss: 2.0168, train_acc: 0.6013, val_acc: 0.4705
[20231203-054133] [INFO] elapsed_time: 102.79 min
[20231203-054506] [INFO] Epoch: 30, train_loss: 1.3712, val_loss: 2.0763, train_acc: 0.6217, val_acc: 0.4535
[20231203-054506] [INFO] elapsed_time: 106.34 min
[20231203-054838] [INFO] Epoch: 31, train_loss: 1.3718, val_loss: 2.2076, train_acc: 0.6246, val_acc: 0.4331
[20231203-054838] [INFO] elapsed_time: 109.87 min
[20231203-055210] [INFO] Epoch: 32, train_loss: 1.3493, val_loss: 2.0348, train_acc: 0.6347, val_acc: 0.4571
[20231203-055210] [INFO] elapsed_time: 113.40 min
[20231203-055542] [INFO] Epoch: 33, train_loss: 1.4487, val_loss: 2.1482, train_acc: 0.6090, val_acc: 0.4367
[20231203-055542] [INFO] elapsed_time: 116.95 min
[20231203-055915] [INFO] Epoch: 34, train_loss: 1.3121, val_loss: 1.9499, train_acc: 0.6477, val_acc: 0.4965
[20231203-055915] [INFO] elapsed_time: 120.49 min
[20231203-060247] [INFO] Epoch: 35, train_loss: 1.2270, val_loss: 1.9433, train_acc: 0.6679, val_acc: 0.4980
[20231203-060247] [INFO] elapsed_time: 124.03 min
[20231203-060620] [INFO] Epoch: 36, train_loss: 1.2052, val_loss: 1.9937, train_acc: 0.6778, val_acc: 0.4722
[20231203-060620] [INFO] elapsed_time: 127.57 min
[20231203-060954] [INFO] Epoch: 37, train_loss: 1.1817, val_loss: 1.9739, train_acc: 0.6812, val_acc: 0.4908
[20231203-060954] [INFO] elapsed_time: 131.14 min
[20231203-061326] [INFO] Epoch: 38, train_loss: 1.1727, val_loss: 1.9765, train_acc: 0.6833, val_acc: 0.4881
[20231203-061326] [INFO] elapsed_time: 134.67 min
[20231203-061658] [INFO] Epoch: 39, train_loss: 1.1779, val_loss: 1.9561, train_acc: 0.6841, val_acc: 0.4947
[20231203-061658] [INFO] elapsed_time: 138.21 min
[20231203-062030] [INFO] Epoch: 40, train_loss: 1.2051, val_loss: 1.9880, train_acc: 0.6733, val_acc: 0.4841
[20231203-062030] [INFO] elapsed_time: 141.75 min
[20231203-062402] [INFO] Epoch: 41, train_loss: 1.1383, val_loss: 1.8697, train_acc: 0.6901, val_acc: 0.5195
[20231203-062402] [INFO] elapsed_time: 145.28 min
[20231203-062734] [INFO] Epoch: 42, train_loss: 1.1072, val_loss: 1.9480, train_acc: 0.7007, val_acc: 0.4951
[20231203-062734] [INFO] elapsed_time: 148.81 min
[20231203-063106] [INFO] Epoch: 43, train_loss: 1.1129, val_loss: 1.8840, train_acc: 0.7032, val_acc: 0.5125
[20231203-063106] [INFO] elapsed_time: 152.35 min
[20231203-063438] [INFO] Epoch: 44, train_loss: 1.0873, val_loss: 2.0312, train_acc: 0.7141, val_acc: 0.4667
[20231203-063438] [INFO] elapsed_time: 155.87 min
[20231203-063809] [INFO] Epoch: 45, train_loss: 1.0854, val_loss: 2.0015, train_acc: 0.7114, val_acc: 0.4893
[20231203-063809] [INFO] elapsed_time: 159.39 min
[20231203-064143] [INFO] Epoch: 46, train_loss: 1.0353, val_loss: 1.8789, train_acc: 0.7257, val_acc: 0.5199
[20231203-064143] [INFO] elapsed_time: 162.96 min
[20231203-064516] [INFO] Epoch: 47, train_loss: 1.0435, val_loss: 1.9296, train_acc: 0.7211, val_acc: 0.5070
[20231203-064516] [INFO] elapsed_time: 166.50 min
[20231203-064850] [INFO] Epoch: 48, train_loss: 0.9865, val_loss: 1.8258, train_acc: 0.7369, val_acc: 0.5304
[20231203-064850] [INFO] elapsed_time: 170.07 min
[20231203-065223] [INFO] Epoch: 49, train_loss: 0.9601, val_loss: 1.8399, train_acc: 0.7436, val_acc: 0.5240
[20231203-065223] [INFO] elapsed_time: 173.63 min
[20231203-065556] [INFO] Epoch: 50, train_loss: 0.9588, val_loss: 1.9379, train_acc: 0.7419, val_acc: 0.5026
[20231203-065556] [INFO] elapsed_time: 177.18 min
[20231203-065928] [INFO] Epoch: 51, train_loss: 0.9565, val_loss: 1.8385, train_acc: 0.7483, val_acc: 0.5282
[20231203-065928] [INFO] elapsed_time: 180.71 min
[20231203-070301] [INFO] Epoch: 52, train_loss: 0.9403, val_loss: 1.8017, train_acc: 0.7496, val_acc: 0.5438
[20231203-070301] [INFO] elapsed_time: 184.25 min
[20231203-070633] [INFO] Epoch: 53, train_loss: 0.9866, val_loss: 2.0281, train_acc: 0.7394, val_acc: 0.4919
[20231203-070633] [INFO] elapsed_time: 187.79 min
[20231203-071005] [INFO] Epoch: 54, train_loss: 0.9969, val_loss: 1.8767, train_acc: 0.7382, val_acc: 0.5229
[20231203-071005] [INFO] elapsed_time: 191.33 min
[20231203-071338] [INFO] Epoch: 55, train_loss: 0.9685, val_loss: 1.8395, train_acc: 0.7445, val_acc: 0.5330
[20231203-071338] [INFO] elapsed_time: 194.88 min
[20231203-071710] [INFO] Epoch: 56, train_loss: 0.9265, val_loss: 1.8525, train_acc: 0.7539, val_acc: 0.5281
[20231203-071710] [INFO] elapsed_time: 198.41 min
[20231203-072043] [INFO] Epoch: 57, train_loss: 0.9294, val_loss: 1.8463, train_acc: 0.7537, val_acc: 0.5265
[20231203-072043] [INFO] elapsed_time: 201.96 min
[20231203-072415] [INFO] Epoch: 58, train_loss: 0.8816, val_loss: 1.7556, train_acc: 0.7633, val_acc: 0.5535
[20231203-072415] [INFO] elapsed_time: 205.50 min
[20231203-072748] [INFO] Epoch: 59, train_loss: 0.9033, val_loss: 1.8501, train_acc: 0.7609, val_acc: 0.5308
[20231203-072748] [INFO] elapsed_time: 209.04 min
[20231203-073122] [INFO] Epoch: 60, train_loss: 0.8779, val_loss: 1.7738, train_acc: 0.7650, val_acc: 0.5495
[20231203-073122] [INFO] elapsed_time: 212.62 min
[20231203-073455] [INFO] Epoch: 61, train_loss: 0.8830, val_loss: 1.8459, train_acc: 0.7686, val_acc: 0.5316
[20231203-073455] [INFO] elapsed_time: 216.16 min
[20231203-073827] [INFO] Epoch: 62, train_loss: 0.8607, val_loss: 1.7810, train_acc: 0.7722, val_acc: 0.5465
[20231203-073827] [INFO] elapsed_time: 219.70 min
[20231203-074200] [INFO] Epoch: 63, train_loss: 0.8565, val_loss: 1.7325, train_acc: 0.7794, val_acc: 0.5604
[20231203-074200] [INFO] elapsed_time: 223.24 min
[20231203-074533] [INFO] Epoch: 64, train_loss: 0.8298, val_loss: 1.7349, train_acc: 0.7860, val_acc: 0.5628
[20231203-074533] [INFO] elapsed_time: 226.80 min
[20231203-074905] [INFO] Epoch: 65, train_loss: 0.8347, val_loss: 1.8531, train_acc: 0.7853, val_acc: 0.5357
[20231203-074905] [INFO] elapsed_time: 230.32 min
[20231203-075236] [INFO] Epoch: 66, train_loss: 0.8378, val_loss: 1.7998, train_acc: 0.7844, val_acc: 0.5432
[20231203-075236] [INFO] elapsed_time: 233.84 min
[20231203-075608] [INFO] Epoch: 67, train_loss: 0.7748, val_loss: 1.7789, train_acc: 0.7928, val_acc: 0.5514
[20231203-075608] [INFO] elapsed_time: 237.37 min
[20231203-075940] [INFO] Epoch: 68, train_loss: 0.8050, val_loss: 1.7893, train_acc: 0.7894, val_acc: 0.5376
[20231203-075940] [INFO] elapsed_time: 240.91 min
[20231203-080313] [INFO] Epoch: 69, train_loss: 0.8402, val_loss: 1.7850, train_acc: 0.7773, val_acc: 0.5506
[20231203-080313] [INFO] elapsed_time: 244.45 min
[20231203-080645] [INFO] Epoch: 70, train_loss: 0.7807, val_loss: 1.8176, train_acc: 0.7963, val_acc: 0.5383
[20231203-080645] [INFO] elapsed_time: 247.99 min
[20231203-081017] [INFO] Epoch: 71, train_loss: 0.8061, val_loss: 1.8116, train_acc: 0.7875, val_acc: 0.5474
[20231203-081017] [INFO] elapsed_time: 251.52 min
[20231203-081349] [INFO] Epoch: 72, train_loss: 0.8066, val_loss: 1.7620, train_acc: 0.7919, val_acc: 0.5581
[20231203-081349] [INFO] elapsed_time: 255.06 min
[20231203-081722] [INFO] Epoch: 73, train_loss: 0.7220, val_loss: 1.8050, train_acc: 0.8100, val_acc: 0.5489
[20231203-081722] [INFO] elapsed_time: 258.61 min
[20231203-082053] [INFO] Epoch: 74, train_loss: 0.7706, val_loss: 1.8832, train_acc: 0.8037, val_acc: 0.5286
[20231203-082053] [INFO] elapsed_time: 262.13 min
[20231203-082426] [INFO] Epoch: 75, train_loss: 0.7730, val_loss: 1.7174, train_acc: 0.8001, val_acc: 0.5673
[20231203-082426] [INFO] elapsed_time: 265.67 min
[20231203-082759] [INFO] Epoch: 76, train_loss: 0.7375, val_loss: 1.7596, train_acc: 0.8059, val_acc: 0.5546
[20231203-082759] [INFO] elapsed_time: 269.23 min
[20231203-083132] [INFO] Epoch: 77, train_loss: 0.7494, val_loss: 1.8004, train_acc: 0.8040, val_acc: 0.5485
[20231203-083132] [INFO] elapsed_time: 272.77 min
[20231203-083503] [INFO] Epoch: 78, train_loss: 0.7157, val_loss: 1.6857, train_acc: 0.8161, val_acc: 0.5770
[20231203-083503] [INFO] elapsed_time: 276.30 min
[20231203-083837] [INFO] Epoch: 79, train_loss: 0.7297, val_loss: 1.7491, train_acc: 0.8153, val_acc: 0.5606
[20231203-083837] [INFO] elapsed_time: 279.85 min
[20231203-084209] [INFO] Epoch: 80, train_loss: 0.7674, val_loss: 1.7935, train_acc: 0.8005, val_acc: 0.5508
[20231203-084209] [INFO] elapsed_time: 283.40 min
[20231203-084542] [INFO] Epoch: 81, train_loss: 0.7302, val_loss: 1.7120, train_acc: 0.8108, val_acc: 0.5693
[20231203-084542] [INFO] elapsed_time: 286.94 min
[20231203-084914] [INFO] Epoch: 82, train_loss: 0.6890, val_loss: 1.6623, train_acc: 0.8223, val_acc: 0.5867
[20231203-084914] [INFO] elapsed_time: 290.47 min
[20231203-085246] [INFO] Epoch: 83, train_loss: 0.7188, val_loss: 1.7956, train_acc: 0.8197, val_acc: 0.5499
[20231203-085246] [INFO] elapsed_time: 294.00 min
[20231203-085618] [INFO] Epoch: 84, train_loss: 0.6621, val_loss: 1.7545, train_acc: 0.8293, val_acc: 0.5654
[20231203-085618] [INFO] elapsed_time: 297.54 min
[20231203-085951] [INFO] Epoch: 85, train_loss: 0.7204, val_loss: 1.7237, train_acc: 0.8145, val_acc: 0.5722
[20231203-085951] [INFO] elapsed_time: 301.09 min
[20231203-090323] [INFO] Epoch: 86, train_loss: 0.6693, val_loss: 1.7083, train_acc: 0.8289, val_acc: 0.5749
[20231203-090323] [INFO] elapsed_time: 304.62 min
[20231203-090655] [INFO] Epoch: 87, train_loss: 0.6806, val_loss: 1.7043, train_acc: 0.8261, val_acc: 0.5739
[20231203-090655] [INFO] elapsed_time: 308.16 min
[20231203-091027] [INFO] Epoch: 88, train_loss: 0.6784, val_loss: 1.6906, train_acc: 0.8260, val_acc: 0.5813
[20231203-091027] [INFO] elapsed_time: 311.70 min
[20231203-091400] [INFO] Epoch: 89, train_loss: 0.6756, val_loss: 1.7590, train_acc: 0.8264, val_acc: 0.5589
[20231203-091400] [INFO] elapsed_time: 315.24 min
[20231203-091733] [INFO] Epoch: 90, train_loss: 0.6811, val_loss: 1.6920, train_acc: 0.8287, val_acc: 0.5790
[20231203-091733] [INFO] elapsed_time: 318.79 min
[20231203-092106] [INFO] Epoch: 91, train_loss: 0.6506, val_loss: 1.7813, train_acc: 0.8352, val_acc: 0.5580
[20231203-092106] [INFO] elapsed_time: 322.35 min
[20231203-092438] [INFO] Epoch: 92, train_loss: 0.6619, val_loss: 1.7044, train_acc: 0.8303, val_acc: 0.5741
[20231203-092438] [INFO] elapsed_time: 325.88 min
[20231203-092811] [INFO] Epoch: 93, train_loss: 0.6796, val_loss: 1.7867, train_acc: 0.8233, val_acc: 0.5537
[20231203-092811] [INFO] elapsed_time: 329.42 min
[20231203-093145] [INFO] Epoch: 94, train_loss: 0.6643, val_loss: 1.7062, train_acc: 0.8314, val_acc: 0.5777
[20231203-093145] [INFO] elapsed_time: 332.99 min
[20231203-093517] [INFO] Epoch: 95, train_loss: 0.6524, val_loss: 1.6820, train_acc: 0.8332, val_acc: 0.5809
[20231203-093517] [INFO] elapsed_time: 336.52 min
[20231203-093849] [INFO] Epoch: 96, train_loss: 0.6442, val_loss: 1.6745, train_acc: 0.8342, val_acc: 0.5830
[20231203-093849] [INFO] elapsed_time: 340.06 min
[20231203-094221] [INFO] Epoch: 97, train_loss: 0.6383, val_loss: 1.6158, train_acc: 0.8413, val_acc: 0.6001
[20231203-094221] [INFO] elapsed_time: 343.59 min
[20231203-094554] [INFO] Epoch: 98, train_loss: 0.6223, val_loss: 1.7003, train_acc: 0.8406, val_acc: 0.5740
[20231203-094554] [INFO] elapsed_time: 347.15 min
[20231203-094927] [INFO] Epoch: 99, train_loss: 0.7029, val_loss: 1.6979, train_acc: 0.8275, val_acc: 0.5750
[20231203-094927] [INFO] elapsed_time: 350.69 min
[20231203-095259] [INFO] Epoch: 100, train_loss: 0.6250, val_loss: 1.6538, train_acc: 0.8367, val_acc: 0.5841
[20231203-095259] [INFO] elapsed_time: 354.23 min
[20231203-095300] [INFO] argument
[20231203-095300] [INFO]   note: None
[20231203-095300] [INFO]   seed: 24771
[20231203-095300] [INFO]   dir_result: ViT/lr3e-5_b256
[20231203-095300] [INFO]   model_name: ViT
[20231203-095300] [INFO]   num_epoch: 100
[20231203-095300] [INFO]   batch_size: 256
[20231203-095300] [INFO]   lr: 3e-05
[20231203-095300] [INFO]   patience: 25
[20231203-095300] [INFO]   delta: 0.002
[20231203-095300] [INFO]   lr_min: 1e-05
[20231203-095300] [INFO]   warmup_t: 10
[20231203-095300] [INFO]   warmup_lr_init: 1e-05
[20231203-095300] [INFO] loss
[20231203-095300] [INFO]   training: True
[20231203-095300] [INFO]   reduction: mean
[20231203-095300] [INFO]   ignore_index: -100
[20231203-095300] [INFO]   label_smoothing: 0.0
[20231203-095300] [INFO] optimizer
[20231203-095300] [INFO]   defaults: {'lr': 3e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}
[20231203-095300] [INFO] scheduler
[20231203-095300] [INFO]   optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 3e-05
    lr: 1.0591192310457746e-05
    maximize: False
    weight_decay: 0
)
[20231203-095300] [INFO]   param_group_field: lr
[20231203-095300] [INFO]   base_values: [3e-05]
[20231203-095300] [INFO]   metric: None
[20231203-095300] [INFO]   t_in_epochs: True
[20231203-095300] [INFO]   noise_range_t: None
[20231203-095300] [INFO]   noise_pct: 0.67
[20231203-095300] [INFO]   noise_type: normal
[20231203-095300] [INFO]   noise_std: 1.0
[20231203-095300] [INFO]   noise_seed: 42
[20231203-095300] [INFO]   t_initial: 100
[20231203-095300] [INFO]   lr_min: 1e-05
[20231203-095300] [INFO]   cycle_mul: 1.0
[20231203-095300] [INFO]   cycle_decay: 1.0
[20231203-095300] [INFO]   cycle_limit: 1
[20231203-095300] [INFO]   warmup_t: 10
[20231203-095300] [INFO]   warmup_lr_init: 1e-05
[20231203-095300] [INFO]   warmup_prefix: True
[20231203-095300] [INFO]   k_decay: 1.0
[20231203-095300] [INFO]   warmup_steps: [2e-06]
