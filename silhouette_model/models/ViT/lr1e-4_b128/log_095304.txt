[20231203-095314] [INFO] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
[20231203-095314] [INFO] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
[20231203-095651] [INFO] Epoch: 1, train_loss: 6.8465, val_loss: 6.7407, train_acc: 0.0013, val_acc: 0.0046
[20231203-095651] [INFO] elapsed_time: 3.60 min
[20231203-100023] [INFO] Epoch: 2, train_loss: 6.5761, val_loss: 6.1453, train_acc: 0.0066, val_acc: 0.0147
[20231203-100023] [INFO] elapsed_time: 7.13 min
[20231203-100355] [INFO] Epoch: 3, train_loss: 5.7392, val_loss: 5.1269, train_acc: 0.0178, val_acc: 0.0301
[20231203-100355] [INFO] elapsed_time: 10.65 min
[20231203-100727] [INFO] Epoch: 4, train_loss: 4.9081, val_loss: 4.4616, train_acc: 0.0342, val_acc: 0.0579
[20231203-100727] [INFO] elapsed_time: 14.20 min
[20231203-101100] [INFO] Epoch: 5, train_loss: 4.3632, val_loss: 4.0083, train_acc: 0.0595, val_acc: 0.0911
[20231203-101100] [INFO] elapsed_time: 17.74 min
[20231203-101432] [INFO] Epoch: 6, train_loss: 3.9717, val_loss: 3.9138, train_acc: 0.0896, val_acc: 0.1041
[20231203-101432] [INFO] elapsed_time: 21.27 min
[20231203-101804] [INFO] Epoch: 7, train_loss: 3.7168, val_loss: 3.4575, train_acc: 0.1237, val_acc: 0.1630
[20231203-101804] [INFO] elapsed_time: 24.81 min
[20231203-102137] [INFO] Epoch: 8, train_loss: 3.4180, val_loss: 3.4196, train_acc: 0.1699, val_acc: 0.1616
[20231203-102137] [INFO] elapsed_time: 28.36 min
[20231203-102510] [INFO] Epoch: 9, train_loss: 3.1902, val_loss: 3.3416, train_acc: 0.2018, val_acc: 0.1951
[20231203-102510] [INFO] elapsed_time: 31.91 min
[20231203-102842] [INFO] Epoch: 10, train_loss: 3.1794, val_loss: 2.9995, train_acc: 0.2105, val_acc: 0.2427
[20231203-102842] [INFO] elapsed_time: 35.44 min
[20231203-103213] [INFO] Epoch: 11, train_loss: 2.9061, val_loss: 2.8562, train_acc: 0.2563, val_acc: 0.2722
[20231203-103213] [INFO] elapsed_time: 38.97 min
[20231203-103547] [INFO] Epoch: 12, train_loss: 2.7388, val_loss: 2.9830, train_acc: 0.2883, val_acc: 0.2514
[20231203-103547] [INFO] elapsed_time: 42.52 min
[20231203-103918] [INFO] Epoch: 13, train_loss: 2.6101, val_loss: 2.6260, train_acc: 0.3171, val_acc: 0.3170
[20231203-103918] [INFO] elapsed_time: 46.05 min
[20231203-104250] [INFO] Epoch: 14, train_loss: 2.4056, val_loss: 2.6539, train_acc: 0.3559, val_acc: 0.3117
[20231203-104250] [INFO] elapsed_time: 49.58 min
[20231203-104622] [INFO] Epoch: 15, train_loss: 2.3188, val_loss: 2.5422, train_acc: 0.3742, val_acc: 0.3387
[20231203-104622] [INFO] elapsed_time: 53.11 min
[20231203-104954] [INFO] Epoch: 16, train_loss: 2.2425, val_loss: 2.6171, train_acc: 0.3983, val_acc: 0.3357
[20231203-104954] [INFO] elapsed_time: 56.65 min
[20231203-105326] [INFO] Epoch: 17, train_loss: 2.0251, val_loss: 2.6886, train_acc: 0.4473, val_acc: 0.3270
[20231203-105326] [INFO] elapsed_time: 60.17 min
[20231203-105658] [INFO] Epoch: 18, train_loss: 2.0093, val_loss: 2.4374, train_acc: 0.4521, val_acc: 0.3709
[20231203-105658] [INFO] elapsed_time: 63.71 min
[20231203-110031] [INFO] Epoch: 19, train_loss: 1.9916, val_loss: 2.6262, train_acc: 0.4659, val_acc: 0.3467
[20231203-110031] [INFO] elapsed_time: 67.26 min
[20231203-110403] [INFO] Epoch: 20, train_loss: 1.9243, val_loss: 2.5773, train_acc: 0.4758, val_acc: 0.3592
[20231203-110403] [INFO] elapsed_time: 70.79 min
[20231203-110736] [INFO] Epoch: 21, train_loss: 1.7986, val_loss: 2.5367, train_acc: 0.5044, val_acc: 0.3679
[20231203-110736] [INFO] elapsed_time: 74.35 min
[20231203-111109] [INFO] Epoch: 22, train_loss: 1.7981, val_loss: 2.3909, train_acc: 0.5134, val_acc: 0.3920
[20231203-111109] [INFO] elapsed_time: 77.89 min
[20231203-111442] [INFO] Epoch: 23, train_loss: 1.6032, val_loss: 2.2780, train_acc: 0.5611, val_acc: 0.4187
[20231203-111442] [INFO] elapsed_time: 81.44 min
[20231203-111814] [INFO] Epoch: 24, train_loss: 1.6187, val_loss: 2.4598, train_acc: 0.5525, val_acc: 0.3777
[20231203-111814] [INFO] elapsed_time: 84.99 min
[20231203-112146] [INFO] Epoch: 25, train_loss: 1.5485, val_loss: 2.3811, train_acc: 0.5771, val_acc: 0.4111
[20231203-112146] [INFO] elapsed_time: 88.52 min
[20231203-112518] [INFO] Epoch: 26, train_loss: 1.4950, val_loss: 2.4190, train_acc: 0.5824, val_acc: 0.4050
[20231203-112518] [INFO] elapsed_time: 92.05 min
[20231203-112850] [INFO] Epoch: 27, train_loss: 1.4857, val_loss: 2.4641, train_acc: 0.5919, val_acc: 0.3930
[20231203-112850] [INFO] elapsed_time: 95.58 min
[20231203-113223] [INFO] Epoch: 28, train_loss: 1.4627, val_loss: 2.3544, train_acc: 0.6018, val_acc: 0.4217
[20231203-113223] [INFO] elapsed_time: 99.13 min
[20231203-113556] [INFO] Epoch: 29, train_loss: 1.4631, val_loss: 2.3843, train_acc: 0.6027, val_acc: 0.4139
[20231203-113556] [INFO] elapsed_time: 102.68 min
[20231203-113928] [INFO] Epoch: 30, train_loss: 1.3340, val_loss: 2.4133, train_acc: 0.6395, val_acc: 0.4129
[20231203-113928] [INFO] elapsed_time: 106.21 min
[20231203-114300] [INFO] Epoch: 31, train_loss: 1.3655, val_loss: 2.2840, train_acc: 0.6253, val_acc: 0.4311
[20231203-114300] [INFO] elapsed_time: 109.75 min
[20231203-114632] [INFO] Epoch: 32, train_loss: 1.2982, val_loss: 2.3220, train_acc: 0.6465, val_acc: 0.4302
[20231203-114632] [INFO] elapsed_time: 113.28 min
[20231203-115005] [INFO] Epoch: 33, train_loss: 1.3735, val_loss: 2.3173, train_acc: 0.6297, val_acc: 0.4330
[20231203-115005] [INFO] elapsed_time: 116.82 min
[20231203-115337] [INFO] Epoch: 34, train_loss: 1.2788, val_loss: 2.3888, train_acc: 0.6560, val_acc: 0.4240
[20231203-115337] [INFO] elapsed_time: 120.36 min
[20231203-115709] [INFO] Epoch: 35, train_loss: 1.2439, val_loss: 2.3347, train_acc: 0.6691, val_acc: 0.4310
[20231203-115709] [INFO] elapsed_time: 123.89 min
[20231203-120042] [INFO] Epoch: 36, train_loss: 1.1595, val_loss: 2.3525, train_acc: 0.6830, val_acc: 0.4247
[20231203-120042] [INFO] elapsed_time: 127.44 min
[20231203-120414] [INFO] Epoch: 37, train_loss: 1.1910, val_loss: 2.5137, train_acc: 0.6844, val_acc: 0.4147
[20231203-120414] [INFO] elapsed_time: 130.98 min
[20231203-120746] [INFO] Epoch: 38, train_loss: 1.1780, val_loss: 2.5624, train_acc: 0.6834, val_acc: 0.3920
[20231203-120746] [INFO] elapsed_time: 134.51 min
[20231203-121118] [INFO] Epoch: 39, train_loss: 1.2169, val_loss: 2.3408, train_acc: 0.6713, val_acc: 0.4381
[20231203-121118] [INFO] elapsed_time: 138.04 min
[20231203-121450] [INFO] Epoch: 40, train_loss: 1.1148, val_loss: 2.3582, train_acc: 0.6970, val_acc: 0.4404
[20231203-121450] [INFO] elapsed_time: 141.58 min
[20231203-121823] [INFO] Epoch: 41, train_loss: 1.1041, val_loss: 2.3218, train_acc: 0.7044, val_acc: 0.4538
[20231203-121823] [INFO] elapsed_time: 145.13 min
[20231203-122155] [INFO] Epoch: 42, train_loss: 1.1383, val_loss: 2.5912, train_acc: 0.6958, val_acc: 0.4025
[20231203-122155] [INFO] elapsed_time: 148.66 min
[20231203-122527] [INFO] Epoch: 43, train_loss: 1.0985, val_loss: 2.2356, train_acc: 0.7057, val_acc: 0.4658
[20231203-122527] [INFO] elapsed_time: 152.19 min
[20231203-122900] [INFO] Epoch: 44, train_loss: 1.0731, val_loss: 2.7533, train_acc: 0.7138, val_acc: 0.3777
[20231203-122900] [INFO] elapsed_time: 155.74 min
[20231203-123231] [INFO] Epoch: 45, train_loss: 1.0625, val_loss: 2.2843, train_acc: 0.7210, val_acc: 0.4560
[20231203-123231] [INFO] elapsed_time: 159.27 min
[20231203-123604] [INFO] Epoch: 46, train_loss: 1.0065, val_loss: 2.2583, train_acc: 0.7315, val_acc: 0.4663
[20231203-123604] [INFO] elapsed_time: 162.80 min
[20231203-123935] [INFO] Epoch: 47, train_loss: 1.0054, val_loss: 2.3176, train_acc: 0.7329, val_acc: 0.4533
[20231203-123935] [INFO] elapsed_time: 166.33 min
[20231203-124307] [INFO] Epoch: 48, train_loss: 0.9918, val_loss: 2.1992, train_acc: 0.7354, val_acc: 0.4736
[20231203-124307] [INFO] elapsed_time: 169.87 min
[20231203-124640] [INFO] Epoch: 49, train_loss: 0.9755, val_loss: 2.4300, train_acc: 0.7417, val_acc: 0.4345
[20231203-124640] [INFO] elapsed_time: 173.41 min
[20231203-125010] [INFO] Epoch: 50, train_loss: 0.9729, val_loss: 2.3596, train_acc: 0.7348, val_acc: 0.4388
[20231203-125010] [INFO] elapsed_time: 176.92 min
[20231203-125342] [INFO] Epoch: 51, train_loss: 0.9604, val_loss: 2.3139, train_acc: 0.7467, val_acc: 0.4526
[20231203-125342] [INFO] elapsed_time: 180.45 min
[20231203-125715] [INFO] Epoch: 52, train_loss: 0.9511, val_loss: 2.2320, train_acc: 0.7456, val_acc: 0.4741
[20231203-125715] [INFO] elapsed_time: 183.99 min
[20231203-130047] [INFO] Epoch: 53, train_loss: 0.9201, val_loss: 2.1326, train_acc: 0.7589, val_acc: 0.4884
[20231203-130047] [INFO] elapsed_time: 187.54 min
[20231203-130420] [INFO] Epoch: 54, train_loss: 0.9414, val_loss: 2.3313, train_acc: 0.7488, val_acc: 0.4515
[20231203-130420] [INFO] elapsed_time: 191.08 min
[20231203-130753] [INFO] Epoch: 55, train_loss: 0.9220, val_loss: 2.2001, train_acc: 0.7605, val_acc: 0.4879
[20231203-130753] [INFO] elapsed_time: 194.63 min
[20231203-131125] [INFO] Epoch: 56, train_loss: 0.9212, val_loss: 2.4464, train_acc: 0.7566, val_acc: 0.4340
[20231203-131125] [INFO] elapsed_time: 198.17 min
[20231203-131458] [INFO] Epoch: 57, train_loss: 0.9185, val_loss: 2.2900, train_acc: 0.7551, val_acc: 0.4660
[20231203-131458] [INFO] elapsed_time: 201.71 min
[20231203-131830] [INFO] Epoch: 58, train_loss: 0.8700, val_loss: 2.2902, train_acc: 0.7677, val_acc: 0.4684
[20231203-131830] [INFO] elapsed_time: 205.24 min
[20231203-132201] [INFO] Epoch: 59, train_loss: 0.8649, val_loss: 2.2780, train_acc: 0.7720, val_acc: 0.4714
[20231203-132201] [INFO] elapsed_time: 208.76 min
[20231203-132533] [INFO] Epoch: 60, train_loss: 0.8495, val_loss: 2.1423, train_acc: 0.7801, val_acc: 0.4996
[20231203-132533] [INFO] elapsed_time: 212.30 min
[20231203-132907] [INFO] Epoch: 61, train_loss: 0.8183, val_loss: 2.1002, train_acc: 0.7873, val_acc: 0.5058
[20231203-132907] [INFO] elapsed_time: 215.86 min
[20231203-133240] [INFO] Epoch: 62, train_loss: 0.8503, val_loss: 2.1456, train_acc: 0.7754, val_acc: 0.4980
[20231203-133240] [INFO] elapsed_time: 219.41 min
[20231203-133613] [INFO] Epoch: 63, train_loss: 0.8637, val_loss: 2.1782, train_acc: 0.7724, val_acc: 0.4942
[20231203-133613] [INFO] elapsed_time: 222.97 min
[20231203-133946] [INFO] Epoch: 64, train_loss: 0.7922, val_loss: 2.1270, train_acc: 0.7992, val_acc: 0.5054
[20231203-133946] [INFO] elapsed_time: 226.51 min
[20231203-134318] [INFO] Epoch: 65, train_loss: 0.8043, val_loss: 2.1864, train_acc: 0.7908, val_acc: 0.4810
[20231203-134318] [INFO] elapsed_time: 230.04 min
[20231203-134650] [INFO] Epoch: 66, train_loss: 0.8238, val_loss: 2.1677, train_acc: 0.7883, val_acc: 0.4948
[20231203-134650] [INFO] elapsed_time: 233.58 min
[20231203-135023] [INFO] Epoch: 67, train_loss: 0.7435, val_loss: 2.0988, train_acc: 0.8034, val_acc: 0.5117
[20231203-135023] [INFO] elapsed_time: 237.12 min
[20231203-135354] [INFO] Epoch: 68, train_loss: 0.7503, val_loss: 2.1653, train_acc: 0.8115, val_acc: 0.4906
[20231203-135354] [INFO] elapsed_time: 240.65 min
[20231203-135726] [INFO] Epoch: 69, train_loss: 0.7393, val_loss: 2.1708, train_acc: 0.8113, val_acc: 0.4837
[20231203-135726] [INFO] elapsed_time: 244.18 min
[20231203-140058] [INFO] Epoch: 70, train_loss: 0.7569, val_loss: 2.2625, train_acc: 0.8077, val_acc: 0.4700
[20231203-140058] [INFO] elapsed_time: 247.72 min
[20231203-140430] [INFO] Epoch: 71, train_loss: 0.7697, val_loss: 2.2306, train_acc: 0.8042, val_acc: 0.4856
[20231203-140430] [INFO] elapsed_time: 251.24 min
[20231203-140801] [INFO] Epoch: 72, train_loss: 0.7138, val_loss: 2.0653, train_acc: 0.8215, val_acc: 0.5158
[20231203-140801] [INFO] elapsed_time: 254.76 min
[20231203-141132] [INFO] Epoch: 73, train_loss: 0.6840, val_loss: 2.1509, train_acc: 0.8222, val_acc: 0.4920
[20231203-141132] [INFO] elapsed_time: 258.28 min
[20231203-141504] [INFO] Epoch: 74, train_loss: 0.6846, val_loss: 2.1216, train_acc: 0.8265, val_acc: 0.5033
[20231203-141504] [INFO] elapsed_time: 261.81 min
[20231203-141835] [INFO] Epoch: 75, train_loss: 0.7036, val_loss: 2.0739, train_acc: 0.8193, val_acc: 0.5152
[20231203-141835] [INFO] elapsed_time: 265.34 min
[20231203-142207] [INFO] Epoch: 76, train_loss: 0.6604, val_loss: 2.0351, train_acc: 0.8312, val_acc: 0.5205
[20231203-142207] [INFO] elapsed_time: 268.87 min
[20231203-142539] [INFO] Epoch: 77, train_loss: 0.6747, val_loss: 2.0718, train_acc: 0.8261, val_acc: 0.5141
[20231203-142539] [INFO] elapsed_time: 272.40 min
[20231203-142911] [INFO] Epoch: 78, train_loss: 0.6508, val_loss: 2.0048, train_acc: 0.8338, val_acc: 0.5288
[20231203-142911] [INFO] elapsed_time: 275.92 min
[20231203-143243] [INFO] Epoch: 79, train_loss: 0.6624, val_loss: 2.0543, train_acc: 0.8354, val_acc: 0.5169
[20231203-143243] [INFO] elapsed_time: 279.46 min
[20231203-143616] [INFO] Epoch: 80, train_loss: 0.6634, val_loss: 2.0354, train_acc: 0.8297, val_acc: 0.5222
[20231203-143616] [INFO] elapsed_time: 283.01 min
[20231203-143948] [INFO] Epoch: 81, train_loss: 0.6497, val_loss: 1.9957, train_acc: 0.8338, val_acc: 0.5279
[20231203-143948] [INFO] elapsed_time: 286.55 min
[20231203-144321] [INFO] Epoch: 82, train_loss: 0.6349, val_loss: 1.9930, train_acc: 0.8378, val_acc: 0.5352
[20231203-144321] [INFO] elapsed_time: 290.10 min
[20231203-144653] [INFO] Epoch: 83, train_loss: 0.6411, val_loss: 2.0277, train_acc: 0.8419, val_acc: 0.5240
[20231203-144653] [INFO] elapsed_time: 293.63 min
[20231203-145025] [INFO] Epoch: 84, train_loss: 0.5901, val_loss: 2.0232, train_acc: 0.8504, val_acc: 0.5294
[20231203-145025] [INFO] elapsed_time: 297.17 min
[20231203-145358] [INFO] Epoch: 85, train_loss: 0.6386, val_loss: 1.9654, train_acc: 0.8418, val_acc: 0.5373
[20231203-145358] [INFO] elapsed_time: 300.71 min
[20231203-145731] [INFO] Epoch: 86, train_loss: 0.5897, val_loss: 1.9953, train_acc: 0.8568, val_acc: 0.5259
[20231203-145731] [INFO] elapsed_time: 304.26 min
[20231203-150103] [INFO] Epoch: 87, train_loss: 0.6226, val_loss: 1.9096, train_acc: 0.8455, val_acc: 0.5493
[20231203-150103] [INFO] elapsed_time: 307.80 min
[20231203-150435] [INFO] Epoch: 88, train_loss: 0.6022, val_loss: 1.9853, train_acc: 0.8509, val_acc: 0.5327
[20231203-150435] [INFO] elapsed_time: 311.33 min
[20231203-150809] [INFO] Epoch: 89, train_loss: 0.5903, val_loss: 1.9213, train_acc: 0.8550, val_acc: 0.5434
[20231203-150809] [INFO] elapsed_time: 314.89 min
[20231203-151141] [INFO] Epoch: 90, train_loss: 0.6006, val_loss: 1.9340, train_acc: 0.8519, val_acc: 0.5442
[20231203-151141] [INFO] elapsed_time: 318.42 min
[20231203-151513] [INFO] Epoch: 91, train_loss: 0.5695, val_loss: 1.9138, train_acc: 0.8631, val_acc: 0.5499
[20231203-151513] [INFO] elapsed_time: 321.96 min
[20231203-151845] [INFO] Epoch: 92, train_loss: 0.5658, val_loss: 1.9246, train_acc: 0.8581, val_acc: 0.5406
[20231203-151845] [INFO] elapsed_time: 325.49 min
[20231203-152216] [INFO] Epoch: 93, train_loss: 0.5546, val_loss: 1.8952, train_acc: 0.8601, val_acc: 0.5507
[20231203-152216] [INFO] elapsed_time: 329.02 min
[20231203-152550] [INFO] Epoch: 94, train_loss: 0.5586, val_loss: 1.8688, train_acc: 0.8659, val_acc: 0.5560
[20231203-152550] [INFO] elapsed_time: 332.58 min
[20231203-152925] [INFO] Epoch: 95, train_loss: 0.5616, val_loss: 1.8660, train_acc: 0.8618, val_acc: 0.5551
[20231203-152925] [INFO] elapsed_time: 336.16 min
[20231203-153257] [INFO] Epoch: 96, train_loss: 0.5468, val_loss: 1.8510, train_acc: 0.8661, val_acc: 0.5602
[20231203-153257] [INFO] elapsed_time: 339.69 min
[20231203-153629] [INFO] Epoch: 97, train_loss: 0.5555, val_loss: 1.8315, train_acc: 0.8653, val_acc: 0.5627
[20231203-153629] [INFO] elapsed_time: 343.23 min
[20231203-154001] [INFO] Epoch: 98, train_loss: 0.5288, val_loss: 1.8399, train_acc: 0.8711, val_acc: 0.5600
[20231203-154001] [INFO] elapsed_time: 346.77 min
[20231203-154333] [INFO] Epoch: 99, train_loss: 0.5673, val_loss: 1.8533, train_acc: 0.8672, val_acc: 0.5556
[20231203-154333] [INFO] elapsed_time: 350.30 min
[20231203-154706] [INFO] Epoch: 100, train_loss: 0.5339, val_loss: 1.8056, train_acc: 0.8681, val_acc: 0.5660
[20231203-154706] [INFO] elapsed_time: 353.85 min
[20231203-154708] [INFO] argument
[20231203-154708] [INFO]   note: None
[20231203-154708] [INFO]   seed: 24771
[20231203-154708] [INFO]   dir_result: ViT/lr1e-4_b256
[20231203-154708] [INFO]   model_name: ViT
[20231203-154708] [INFO]   num_epoch: 100
[20231203-154708] [INFO]   batch_size: 256
[20231203-154708] [INFO]   lr: 0.0001
[20231203-154708] [INFO]   patience: 25
[20231203-154708] [INFO]   delta: 0.002
[20231203-154708] [INFO]   lr_min: 1e-05
[20231203-154708] [INFO]   warmup_t: 10
[20231203-154708] [INFO]   warmup_lr_init: 1e-05
[20231203-154708] [INFO] loss
[20231203-154708] [INFO]   training: True
[20231203-154708] [INFO]   reduction: mean
[20231203-154708] [INFO]   ignore_index: -100
[20231203-154708] [INFO]   label_smoothing: 0.0
[20231203-154708] [INFO] optimizer
[20231203-154708] [INFO]   defaults: {'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}
[20231203-154708] [INFO] scheduler
[20231203-154708] [INFO]   optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.0001
    lr: 1.2660365397059856e-05
    maximize: False
    weight_decay: 0
)
[20231203-154708] [INFO]   param_group_field: lr
[20231203-154708] [INFO]   base_values: [0.0001]
[20231203-154708] [INFO]   metric: None
[20231203-154708] [INFO]   t_in_epochs: True
[20231203-154708] [INFO]   noise_range_t: None
[20231203-154708] [INFO]   noise_pct: 0.67
[20231203-154708] [INFO]   noise_type: normal
[20231203-154708] [INFO]   noise_std: 1.0
[20231203-154708] [INFO]   noise_seed: 42
[20231203-154708] [INFO]   t_initial: 100
[20231203-154708] [INFO]   lr_min: 1e-05
[20231203-154708] [INFO]   cycle_mul: 1.0
[20231203-154708] [INFO]   cycle_decay: 1.0
[20231203-154708] [INFO]   cycle_limit: 1
[20231203-154708] [INFO]   warmup_t: 10
[20231203-154708] [INFO]   warmup_lr_init: 1e-05
[20231203-154708] [INFO]   warmup_prefix: True
[20231203-154708] [INFO]   k_decay: 1.0
[20231203-154708] [INFO]   warmup_steps: [9e-06]
