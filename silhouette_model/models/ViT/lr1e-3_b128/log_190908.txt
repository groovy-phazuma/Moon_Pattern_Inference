[20231203-190918] [INFO] Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
[20231203-190919] [INFO] [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
[20231203-191256] [INFO] Epoch: 1, train_loss: 6.8476, val_loss: 6.7520, train_acc: 0.0008, val_acc: 0.0043
[20231203-191256] [INFO] elapsed_time: 3.60 min
[20231203-191628] [INFO] Epoch: 2, train_loss: 6.5883, val_loss: 6.1835, train_acc: 0.0062, val_acc: 0.0135
[20231203-191628] [INFO] elapsed_time: 7.14 min
[20231203-192001] [INFO] Epoch: 3, train_loss: 5.8620, val_loss: 5.0285, train_acc: 0.0120, val_acc: 0.0269
[20231203-192001] [INFO] elapsed_time: 10.68 min
[20231203-192334] [INFO] Epoch: 4, train_loss: 5.0134, val_loss: 4.5816, train_acc: 0.0264, val_acc: 0.0462
[20231203-192334] [INFO] elapsed_time: 14.24 min
[20231203-192707] [INFO] Epoch: 5, train_loss: 4.6612, val_loss: 4.3653, train_acc: 0.0354, val_acc: 0.0467
[20231203-192707] [INFO] elapsed_time: 17.78 min
[20231203-193039] [INFO] Epoch: 6, train_loss: 4.3655, val_loss: 4.2654, train_acc: 0.0471, val_acc: 0.0585
[20231203-193039] [INFO] elapsed_time: 21.31 min
[20231203-193411] [INFO] Epoch: 7, train_loss: 4.3098, val_loss: 3.9967, train_acc: 0.0498, val_acc: 0.0743
[20231203-193411] [INFO] elapsed_time: 24.85 min
[20231203-193743] [INFO] Epoch: 8, train_loss: 4.1765, val_loss: 3.9880, train_acc: 0.0608, val_acc: 0.0798
[20231203-193743] [INFO] elapsed_time: 28.38 min
[20231203-194115] [INFO] Epoch: 9, train_loss: 4.0212, val_loss: 3.7653, train_acc: 0.0722, val_acc: 0.0959
[20231203-194115] [INFO] elapsed_time: 31.92 min
[20231203-194448] [INFO] Epoch: 10, train_loss: 3.9323, val_loss: 3.9405, train_acc: 0.0745, val_acc: 0.0828
[20231203-194448] [INFO] elapsed_time: 35.47 min
[20231203-194819] [INFO] Epoch: 11, train_loss: 3.9081, val_loss: 3.8135, train_acc: 0.0783, val_acc: 0.0951
[20231203-194819] [INFO] elapsed_time: 38.98 min
[20231203-195151] [INFO] Epoch: 12, train_loss: 3.9870, val_loss: 3.8240, train_acc: 0.0765, val_acc: 0.0954
[20231203-195151] [INFO] elapsed_time: 42.51 min
[20231203-195523] [INFO] Epoch: 13, train_loss: 3.7269, val_loss: 3.6343, train_acc: 0.0935, val_acc: 0.1059
[20231203-195523] [INFO] elapsed_time: 46.05 min
[20231203-195856] [INFO] Epoch: 14, train_loss: 3.5770, val_loss: 3.4715, train_acc: 0.0996, val_acc: 0.1300
[20231203-195856] [INFO] elapsed_time: 49.59 min
[20231203-200228] [INFO] Epoch: 15, train_loss: 3.4656, val_loss: 3.4049, train_acc: 0.1155, val_acc: 0.1366
[20231203-200228] [INFO] elapsed_time: 53.13 min
[20231203-200600] [INFO] Epoch: 16, train_loss: 3.4023, val_loss: 3.3392, train_acc: 0.1218, val_acc: 0.1406
[20231203-200600] [INFO] elapsed_time: 56.67 min
[20231203-200932] [INFO] Epoch: 17, train_loss: 3.4057, val_loss: 3.5980, train_acc: 0.1225, val_acc: 0.1088
[20231203-200932] [INFO] elapsed_time: 60.20 min
[20231203-201303] [INFO] Epoch: 18, train_loss: 3.2993, val_loss: 3.4236, train_acc: 0.1291, val_acc: 0.1288
[20231203-201303] [INFO] elapsed_time: 63.72 min
[20231203-201636] [INFO] Epoch: 19, train_loss: 3.2870, val_loss: 3.3288, train_acc: 0.1314, val_acc: 0.1442
[20231203-201636] [INFO] elapsed_time: 67.27 min
[20231203-202010] [INFO] Epoch: 20, train_loss: 3.2402, val_loss: 3.2734, train_acc: 0.1382, val_acc: 0.1514
[20231203-202010] [INFO] elapsed_time: 70.83 min
[20231203-202343] [INFO] Epoch: 21, train_loss: 3.3653, val_loss: 3.4977, train_acc: 0.1249, val_acc: 0.1390
[20231203-202343] [INFO] elapsed_time: 74.38 min
[20231203-202715] [INFO] Epoch: 22, train_loss: 3.2227, val_loss: 3.2158, train_acc: 0.1471, val_acc: 0.1605
[20231203-202715] [INFO] elapsed_time: 77.92 min
[20231203-203047] [INFO] Epoch: 23, train_loss: 3.1772, val_loss: 3.1595, train_acc: 0.1443, val_acc: 0.1686
[20231203-203047] [INFO] elapsed_time: 81.44 min
[20231203-203420] [INFO] Epoch: 24, train_loss: 3.1634, val_loss: 3.0812, train_acc: 0.1503, val_acc: 0.1852
[20231203-203420] [INFO] elapsed_time: 85.00 min
[20231203-203752] [INFO] Epoch: 25, train_loss: 3.0826, val_loss: 3.1046, train_acc: 0.1574, val_acc: 0.1591
[20231203-203752] [INFO] elapsed_time: 88.53 min
[20231203-204124] [INFO] Epoch: 26, train_loss: 3.1326, val_loss: 3.3229, train_acc: 0.1527, val_acc: 0.1374
[20231203-204124] [INFO] elapsed_time: 92.07 min
[20231203-204457] [INFO] Epoch: 27, train_loss: 3.1368, val_loss: 3.1124, train_acc: 0.1522, val_acc: 0.1711
[20231203-204457] [INFO] elapsed_time: 95.61 min
[20231203-204828] [INFO] Epoch: 28, train_loss: 3.0354, val_loss: 3.0335, train_acc: 0.1689, val_acc: 0.1793
[20231203-204828] [INFO] elapsed_time: 99.14 min
[20231203-205201] [INFO] Epoch: 29, train_loss: 2.9666, val_loss: 2.9329, train_acc: 0.1766, val_acc: 0.2170
[20231203-205201] [INFO] elapsed_time: 102.68 min
[20231203-205534] [INFO] Epoch: 30, train_loss: 3.0015, val_loss: 3.0975, train_acc: 0.1717, val_acc: 0.1614
[20231203-205534] [INFO] elapsed_time: 106.23 min
[20231203-205906] [INFO] Epoch: 31, train_loss: 2.9588, val_loss: 3.0010, train_acc: 0.1724, val_acc: 0.1865
[20231203-205906] [INFO] elapsed_time: 109.76 min
[20231203-210238] [INFO] Epoch: 32, train_loss: 2.9157, val_loss: 2.9353, train_acc: 0.1759, val_acc: 0.1924
[20231203-210238] [INFO] elapsed_time: 113.30 min
[20231203-210610] [INFO] Epoch: 33, train_loss: 2.8734, val_loss: 2.9184, train_acc: 0.1875, val_acc: 0.2042
[20231203-210610] [INFO] elapsed_time: 116.83 min
[20231203-210942] [INFO] Epoch: 34, train_loss: 2.8858, val_loss: 3.0284, train_acc: 0.1910, val_acc: 0.1869
[20231203-210942] [INFO] elapsed_time: 120.37 min
[20231203-211315] [INFO] Epoch: 35, train_loss: 2.8475, val_loss: 2.9119, train_acc: 0.1894, val_acc: 0.2167
[20231203-211315] [INFO] elapsed_time: 123.91 min
[20231203-211647] [INFO] Epoch: 36, train_loss: 2.7356, val_loss: 2.9411, train_acc: 0.2050, val_acc: 0.1976
[20231203-211647] [INFO] elapsed_time: 127.46 min
[20231203-212019] [INFO] Epoch: 37, train_loss: 2.7943, val_loss: 2.8029, train_acc: 0.2060, val_acc: 0.2301
[20231203-212019] [INFO] elapsed_time: 130.99 min
[20231203-212352] [INFO] Epoch: 38, train_loss: 2.6963, val_loss: 3.0304, train_acc: 0.2168, val_acc: 0.2016
[20231203-212352] [INFO] elapsed_time: 134.53 min
[20231203-212725] [INFO] Epoch: 39, train_loss: 2.7584, val_loss: 2.9643, train_acc: 0.2186, val_acc: 0.1977
[20231203-212725] [INFO] elapsed_time: 138.08 min
[20231203-213057] [INFO] Epoch: 40, train_loss: 2.7712, val_loss: 2.7902, train_acc: 0.2062, val_acc: 0.2219
[20231203-213057] [INFO] elapsed_time: 141.62 min
[20231203-213429] [INFO] Epoch: 41, train_loss: 2.6882, val_loss: 3.0735, train_acc: 0.2192, val_acc: 0.1855
[20231203-213429] [INFO] elapsed_time: 145.15 min
[20231203-213800] [INFO] Epoch: 42, train_loss: 2.6637, val_loss: 2.7554, train_acc: 0.2235, val_acc: 0.2511
[20231203-213800] [INFO] elapsed_time: 148.68 min
[20231203-214134] [INFO] Epoch: 43, train_loss: 2.6922, val_loss: 2.8191, train_acc: 0.2129, val_acc: 0.2196
[20231203-214134] [INFO] elapsed_time: 152.23 min
[20231203-214506] [INFO] Epoch: 44, train_loss: 2.6774, val_loss: 2.8700, train_acc: 0.2222, val_acc: 0.2001
[20231203-214506] [INFO] elapsed_time: 155.76 min
[20231203-214837] [INFO] Epoch: 45, train_loss: 2.6094, val_loss: 3.0533, train_acc: 0.2301, val_acc: 0.2009
[20231203-214837] [INFO] elapsed_time: 159.29 min
[20231203-215212] [INFO] Epoch: 46, train_loss: 2.5936, val_loss: 2.7889, train_acc: 0.2375, val_acc: 0.2315
[20231203-215212] [INFO] elapsed_time: 162.86 min
[20231203-215544] [INFO] Epoch: 47, train_loss: 2.5264, val_loss: 2.9142, train_acc: 0.2478, val_acc: 0.2103
[20231203-215544] [INFO] elapsed_time: 166.39 min
[20231203-215916] [INFO] Epoch: 48, train_loss: 2.4945, val_loss: 2.6731, train_acc: 0.2640, val_acc: 0.2543
[20231203-215916] [INFO] elapsed_time: 169.93 min
[20231203-220248] [INFO] Epoch: 49, train_loss: 2.5024, val_loss: 2.6148, train_acc: 0.2506, val_acc: 0.2746
[20231203-220248] [INFO] elapsed_time: 173.47 min
[20231203-220620] [INFO] Epoch: 50, train_loss: 2.3899, val_loss: 2.6467, train_acc: 0.2749, val_acc: 0.2660
[20231203-220620] [INFO] elapsed_time: 177.01 min
[20231203-220953] [INFO] Epoch: 51, train_loss: 2.4389, val_loss: 2.7273, train_acc: 0.2707, val_acc: 0.2563
[20231203-220953] [INFO] elapsed_time: 180.55 min
[20231203-221326] [INFO] Epoch: 52, train_loss: 2.4807, val_loss: 2.7389, train_acc: 0.2686, val_acc: 0.2552
[20231203-221326] [INFO] elapsed_time: 184.10 min
[20231203-221658] [INFO] Epoch: 53, train_loss: 2.4430, val_loss: 2.6129, train_acc: 0.2688, val_acc: 0.2740
[20231203-221658] [INFO] elapsed_time: 187.63 min
[20231203-222030] [INFO] Epoch: 54, train_loss: 2.3996, val_loss: 2.6771, train_acc: 0.2772, val_acc: 0.2658
[20231203-222030] [INFO] elapsed_time: 191.17 min
[20231203-222402] [INFO] Epoch: 55, train_loss: 2.4160, val_loss: 2.5801, train_acc: 0.2832, val_acc: 0.3060
[20231203-222402] [INFO] elapsed_time: 194.71 min
[20231203-222735] [INFO] Epoch: 56, train_loss: 2.3143, val_loss: 2.5275, train_acc: 0.3003, val_acc: 0.2837
[20231203-222735] [INFO] elapsed_time: 198.25 min
[20231203-223108] [INFO] Epoch: 57, train_loss: 2.3678, val_loss: 2.6194, train_acc: 0.2859, val_acc: 0.2946
[20231203-223108] [INFO] elapsed_time: 201.79 min
[20231203-223440] [INFO] Epoch: 58, train_loss: 2.2469, val_loss: 2.6460, train_acc: 0.3077, val_acc: 0.2671
[20231203-223440] [INFO] elapsed_time: 205.34 min
[20231203-223811] [INFO] Epoch: 59, train_loss: 2.2234, val_loss: 2.4965, train_acc: 0.3144, val_acc: 0.2986
[20231203-223811] [INFO] elapsed_time: 208.86 min
[20231203-224144] [INFO] Epoch: 60, train_loss: 2.2233, val_loss: 2.4892, train_acc: 0.3119, val_acc: 0.3169
[20231203-224144] [INFO] elapsed_time: 212.41 min
[20231203-224517] [INFO] Epoch: 61, train_loss: 2.1253, val_loss: 2.4173, train_acc: 0.3407, val_acc: 0.3473
[20231203-224517] [INFO] elapsed_time: 215.94 min
[20231203-224849] [INFO] Epoch: 62, train_loss: 2.0890, val_loss: 2.4241, train_acc: 0.3520, val_acc: 0.3380
[20231203-224849] [INFO] elapsed_time: 219.49 min
[20231203-225221] [INFO] Epoch: 63, train_loss: 2.1058, val_loss: 2.4336, train_acc: 0.3380, val_acc: 0.3324
[20231203-225221] [INFO] elapsed_time: 223.03 min
[20231203-225553] [INFO] Epoch: 64, train_loss: 2.1748, val_loss: 2.3517, train_acc: 0.3355, val_acc: 0.3440
[20231203-225553] [INFO] elapsed_time: 226.56 min
[20231203-225926] [INFO] Epoch: 65, train_loss: 2.1515, val_loss: 2.4055, train_acc: 0.3364, val_acc: 0.3254
[20231203-225926] [INFO] elapsed_time: 230.10 min
[20231203-230258] [INFO] Epoch: 66, train_loss: 2.0445, val_loss: 2.3948, train_acc: 0.3606, val_acc: 0.3355
[20231203-230258] [INFO] elapsed_time: 233.63 min
[20231203-230630] [INFO] Epoch: 67, train_loss: 2.0975, val_loss: 2.3991, train_acc: 0.3466, val_acc: 0.3442
[20231203-230630] [INFO] elapsed_time: 237.17 min
[20231203-231002] [INFO] Epoch: 68, train_loss: 1.9877, val_loss: 2.2724, train_acc: 0.3786, val_acc: 0.3617
[20231203-231002] [INFO] elapsed_time: 240.70 min
[20231203-231334] [INFO] Epoch: 69, train_loss: 2.0145, val_loss: 2.2454, train_acc: 0.3686, val_acc: 0.3788
[20231203-231334] [INFO] elapsed_time: 244.24 min
[20231203-231707] [INFO] Epoch: 70, train_loss: 1.9077, val_loss: 2.3346, train_acc: 0.4031, val_acc: 0.3771
[20231203-231707] [INFO] elapsed_time: 247.79 min
[20231203-232039] [INFO] Epoch: 71, train_loss: 1.9215, val_loss: 2.3571, train_acc: 0.3946, val_acc: 0.3696
[20231203-232039] [INFO] elapsed_time: 251.32 min
[20231203-232412] [INFO] Epoch: 72, train_loss: 1.8289, val_loss: 2.2324, train_acc: 0.4202, val_acc: 0.4153
[20231203-232412] [INFO] elapsed_time: 254.86 min
[20231203-232744] [INFO] Epoch: 73, train_loss: 1.7671, val_loss: 2.2530, train_acc: 0.4244, val_acc: 0.4236
[20231203-232744] [INFO] elapsed_time: 258.40 min
[20231203-233116] [INFO] Epoch: 74, train_loss: 1.8688, val_loss: 2.1722, train_acc: 0.4070, val_acc: 0.4157
[20231203-233116] [INFO] elapsed_time: 261.93 min
[20231203-233449] [INFO] Epoch: 75, train_loss: 1.8067, val_loss: 2.2234, train_acc: 0.4285, val_acc: 0.4191
[20231203-233449] [INFO] elapsed_time: 265.48 min
[20231203-233821] [INFO] Epoch: 76, train_loss: 1.6860, val_loss: 2.3187, train_acc: 0.4599, val_acc: 0.4068
[20231203-233821] [INFO] elapsed_time: 269.02 min
[20231203-234152] [INFO] Epoch: 77, train_loss: 1.7422, val_loss: 2.2055, train_acc: 0.4377, val_acc: 0.4167
[20231203-234152] [INFO] elapsed_time: 272.54 min
[20231203-234525] [INFO] Epoch: 78, train_loss: 1.6406, val_loss: 2.2467, train_acc: 0.4746, val_acc: 0.4485
[20231203-234525] [INFO] elapsed_time: 276.08 min
[20231203-234857] [INFO] Epoch: 79, train_loss: 1.6295, val_loss: 2.3567, train_acc: 0.4738, val_acc: 0.4051
[20231203-234857] [INFO] elapsed_time: 279.63 min
[20231203-235229] [INFO] Epoch: 80, train_loss: 1.6578, val_loss: 2.2581, train_acc: 0.4634, val_acc: 0.4352
[20231203-235229] [INFO] elapsed_time: 283.16 min
[20231203-235601] [INFO] Epoch: 81, train_loss: 1.5987, val_loss: 2.1737, train_acc: 0.4814, val_acc: 0.4529
[20231203-235601] [INFO] elapsed_time: 286.69 min
[20231203-235933] [INFO] Epoch: 82, train_loss: 1.5591, val_loss: 2.2217, train_acc: 0.4909, val_acc: 0.4533
[20231203-235933] [INFO] elapsed_time: 290.22 min
[20231204-000305] [INFO] Epoch: 83, train_loss: 1.5148, val_loss: 2.2560, train_acc: 0.5121, val_acc: 0.4710
[20231204-000305] [INFO] elapsed_time: 293.75 min
[20231204-000637] [INFO] Epoch: 84, train_loss: 1.5049, val_loss: 2.1921, train_acc: 0.5086, val_acc: 0.4654
[20231204-000637] [INFO] elapsed_time: 297.29 min
[20231204-001009] [INFO] Epoch: 85, train_loss: 1.4904, val_loss: 2.2095, train_acc: 0.5160, val_acc: 0.4893
[20231204-001009] [INFO] elapsed_time: 300.82 min
[20231204-001342] [INFO] Epoch: 86, train_loss: 1.4770, val_loss: 2.4934, train_acc: 0.5208, val_acc: 0.4069
[20231204-001342] [INFO] elapsed_time: 304.36 min
[20231204-001714] [INFO] Epoch: 87, train_loss: 1.5065, val_loss: 2.1962, train_acc: 0.5114, val_acc: 0.4960
[20231204-001714] [INFO] elapsed_time: 307.90 min
[20231204-002047] [INFO] Epoch: 88, train_loss: 1.4167, val_loss: 2.2560, train_acc: 0.5359, val_acc: 0.4953
[20231204-002047] [INFO] elapsed_time: 311.46 min
[20231204-002420] [INFO] Epoch: 89, train_loss: 1.4032, val_loss: 2.3422, train_acc: 0.5414, val_acc: 0.4918
[20231204-002420] [INFO] elapsed_time: 315.00 min
[20231204-002753] [INFO] Epoch: 90, train_loss: 1.3985, val_loss: 2.3206, train_acc: 0.5470, val_acc: 0.4897
[20231204-002753] [INFO] elapsed_time: 318.55 min
[20231204-003126] [INFO] Epoch: 91, train_loss: 1.3436, val_loss: 2.3279, train_acc: 0.5581, val_acc: 0.5005
[20231204-003126] [INFO] elapsed_time: 322.09 min
[20231204-003458] [INFO] Epoch: 92, train_loss: 1.3239, val_loss: 2.3702, train_acc: 0.5597, val_acc: 0.4992
[20231204-003458] [INFO] elapsed_time: 325.63 min
[20231204-003830] [INFO] Epoch: 93, train_loss: 1.3291, val_loss: 2.3334, train_acc: 0.5588, val_acc: 0.5082
[20231204-003830] [INFO] elapsed_time: 329.17 min
[20231204-004203] [INFO] Epoch: 94, train_loss: 1.3660, val_loss: 2.3173, train_acc: 0.5481, val_acc: 0.5164
[20231204-004203] [INFO] elapsed_time: 332.72 min
[20231204-004536] [INFO] Epoch: 95, train_loss: 1.3021, val_loss: 2.3921, train_acc: 0.5728, val_acc: 0.5025
[20231204-004536] [INFO] elapsed_time: 336.27 min
[20231204-004909] [INFO] Epoch: 96, train_loss: 1.2816, val_loss: 2.3563, train_acc: 0.5790, val_acc: 0.5268
[20231204-004909] [INFO] elapsed_time: 339.82 min
[20231204-005242] [INFO] Epoch: 97, train_loss: 1.2537, val_loss: 2.4109, train_acc: 0.5918, val_acc: 0.5279
[20231204-005242] [INFO] elapsed_time: 343.36 min
[20231204-005614] [INFO] Epoch: 98, train_loss: 1.2341, val_loss: 2.4300, train_acc: 0.5936, val_acc: 0.5284
[20231204-005614] [INFO] elapsed_time: 346.91 min
[20231204-005947] [INFO] Epoch: 99, train_loss: 1.2657, val_loss: 2.4046, train_acc: 0.5837, val_acc: 0.5287
[20231204-005947] [INFO] elapsed_time: 350.45 min
[20231204-005947] [INFO] Early Stopping with Epoch: 98
[20231204-005948] [INFO] argument
[20231204-005948] [INFO]   note: None
[20231204-005948] [INFO]   seed: 24771
[20231204-005948] [INFO]   dir_result: ViT/lr1e-3_b256
[20231204-005948] [INFO]   model_name: ViT
[20231204-005948] [INFO]   num_epoch: 100
[20231204-005948] [INFO]   batch_size: 256
[20231204-005948] [INFO]   lr: 0.001
[20231204-005948] [INFO]   patience: 25
[20231204-005948] [INFO]   delta: 0.002
[20231204-005948] [INFO]   lr_min: 1e-05
[20231204-005948] [INFO]   warmup_t: 10
[20231204-005948] [INFO]   warmup_lr_init: 1e-05
[20231204-005948] [INFO] loss
[20231204-005948] [INFO]   training: True
[20231204-005948] [INFO]   reduction: mean
[20231204-005948] [INFO]   ignore_index: -100
[20231204-005948] [INFO]   label_smoothing: 0.0
[20231204-005948] [INFO] optimizer
[20231204-005948] [INFO]   defaults: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}
[20231204-005948] [INFO] scheduler
[20231204-005948] [INFO]   optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 4.4760639485315584e-05
    maximize: False
    weight_decay: 0
)
[20231204-005948] [INFO]   param_group_field: lr
[20231204-005948] [INFO]   base_values: [0.001]
[20231204-005948] [INFO]   metric: None
[20231204-005948] [INFO]   t_in_epochs: True
[20231204-005948] [INFO]   noise_range_t: None
[20231204-005948] [INFO]   noise_pct: 0.67
[20231204-005948] [INFO]   noise_type: normal
[20231204-005948] [INFO]   noise_std: 1.0
[20231204-005948] [INFO]   noise_seed: 42
[20231204-005948] [INFO]   t_initial: 100
[20231204-005948] [INFO]   lr_min: 1e-05
[20231204-005948] [INFO]   cycle_mul: 1.0
[20231204-005948] [INFO]   cycle_decay: 1.0
[20231204-005948] [INFO]   cycle_limit: 1
[20231204-005948] [INFO]   warmup_t: 10
[20231204-005948] [INFO]   warmup_lr_init: 1e-05
[20231204-005948] [INFO]   warmup_prefix: True
[20231204-005948] [INFO]   k_decay: 1.0
[20231204-005948] [INFO]   warmup_steps: [9.9e-05]
