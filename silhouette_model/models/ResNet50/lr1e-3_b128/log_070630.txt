[20231206-071002] [INFO] Epoch: 1, train_loss: 6.8650, val_loss: 6.8326, train_acc: 0.0010, val_acc: 0.0013
[20231206-071002] [INFO] elapsed_time: 3.51 min
[20231206-071328] [INFO] Epoch: 2, train_loss: 6.7978, val_loss: 6.7741, train_acc: 0.0020, val_acc: 0.0036
[20231206-071328] [INFO] elapsed_time: 6.95 min
[20231206-071655] [INFO] Epoch: 3, train_loss: 6.4403, val_loss: 5.7590, train_acc: 0.0059, val_acc: 0.0156
[20231206-071655] [INFO] elapsed_time: 10.39 min
[20231206-072021] [INFO] Epoch: 4, train_loss: 5.5156, val_loss: 5.7324, train_acc: 0.0160, val_acc: 0.0201
[20231206-072021] [INFO] elapsed_time: 13.83 min
[20231206-072348] [INFO] Epoch: 5, train_loss: 4.9903, val_loss: 4.6112, train_acc: 0.0339, val_acc: 0.0537
[20231206-072348] [INFO] elapsed_time: 17.28 min
[20231206-072715] [INFO] Epoch: 6, train_loss: 4.6247, val_loss: 5.0558, train_acc: 0.0497, val_acc: 0.0439
[20231206-072715] [INFO] elapsed_time: 20.73 min
[20231206-073042] [INFO] Epoch: 7, train_loss: 4.6056, val_loss: 4.7602, train_acc: 0.0546, val_acc: 0.0542
[20231206-073042] [INFO] elapsed_time: 24.18 min
[20231206-073410] [INFO] Epoch: 8, train_loss: 4.3569, val_loss: 4.6443, train_acc: 0.0680, val_acc: 0.0749
[20231206-073410] [INFO] elapsed_time: 27.64 min
[20231206-073737] [INFO] Epoch: 9, train_loss: 4.2375, val_loss: 5.3924, train_acc: 0.0768, val_acc: 0.0564
[20231206-073737] [INFO] elapsed_time: 31.09 min
[20231206-074103] [INFO] Epoch: 10, train_loss: 4.2327, val_loss: 4.1827, train_acc: 0.0781, val_acc: 0.0863
[20231206-074103] [INFO] elapsed_time: 34.53 min
[20231206-074430] [INFO] Epoch: 11, train_loss: 4.0968, val_loss: 5.0159, train_acc: 0.0860, val_acc: 0.0557
[20231206-074430] [INFO] elapsed_time: 37.98 min
[20231206-074757] [INFO] Epoch: 12, train_loss: 4.0339, val_loss: 5.3237, train_acc: 0.0907, val_acc: 0.0600
[20231206-074757] [INFO] elapsed_time: 41.43 min
[20231206-075124] [INFO] Epoch: 13, train_loss: 3.8216, val_loss: 4.0865, train_acc: 0.1148, val_acc: 0.1032
[20231206-075124] [INFO] elapsed_time: 44.87 min
[20231206-075450] [INFO] Epoch: 14, train_loss: 3.7398, val_loss: 3.7961, train_acc: 0.1251, val_acc: 0.1264
[20231206-075450] [INFO] elapsed_time: 48.32 min
[20231206-075817] [INFO] Epoch: 15, train_loss: 3.6001, val_loss: 4.2694, train_acc: 0.1374, val_acc: 0.1074
[20231206-075817] [INFO] elapsed_time: 51.76 min
[20231206-080144] [INFO] Epoch: 16, train_loss: 3.5477, val_loss: 4.9055, train_acc: 0.1389, val_acc: 0.0957
[20231206-080144] [INFO] elapsed_time: 55.21 min
[20231206-080511] [INFO] Epoch: 17, train_loss: 3.4259, val_loss: 7.1245, train_acc: 0.1506, val_acc: 0.0404
[20231206-080511] [INFO] elapsed_time: 58.66 min
[20231206-080838] [INFO] Epoch: 18, train_loss: 3.3740, val_loss: 3.9377, train_acc: 0.1615, val_acc: 0.1570
[20231206-080838] [INFO] elapsed_time: 62.11 min
[20231206-081205] [INFO] Epoch: 19, train_loss: 3.3177, val_loss: 3.8675, train_acc: 0.1734, val_acc: 0.1498
[20231206-081205] [INFO] elapsed_time: 65.57 min
[20231206-081532] [INFO] Epoch: 20, train_loss: 3.2292, val_loss: 4.2367, train_acc: 0.1887, val_acc: 0.1157
[20231206-081532] [INFO] elapsed_time: 69.01 min
[20231206-081859] [INFO] Epoch: 21, train_loss: 3.1252, val_loss: 3.6912, train_acc: 0.2030, val_acc: 0.1692
[20231206-081859] [INFO] elapsed_time: 72.46 min
[20231206-082227] [INFO] Epoch: 22, train_loss: 3.0982, val_loss: 4.5353, train_acc: 0.2091, val_acc: 0.1279
[20231206-082227] [INFO] elapsed_time: 75.92 min
[20231206-082554] [INFO] Epoch: 23, train_loss: 3.0234, val_loss: 5.1117, train_acc: 0.2221, val_acc: 0.1067
[20231206-082554] [INFO] elapsed_time: 79.38 min
[20231206-082921] [INFO] Epoch: 24, train_loss: 3.0196, val_loss: 4.1354, train_acc: 0.2271, val_acc: 0.1445
[20231206-082921] [INFO] elapsed_time: 82.82 min
[20231206-083248] [INFO] Epoch: 25, train_loss: 2.9616, val_loss: 4.8250, train_acc: 0.2302, val_acc: 0.1273
[20231206-083248] [INFO] elapsed_time: 86.27 min
[20231206-083614] [INFO] Epoch: 26, train_loss: 2.8817, val_loss: 5.0389, train_acc: 0.2541, val_acc: 0.1122
[20231206-083614] [INFO] elapsed_time: 89.72 min
[20231206-083942] [INFO] Epoch: 27, train_loss: 2.9212, val_loss: 7.1044, train_acc: 0.2427, val_acc: 0.0869
[20231206-083942] [INFO] elapsed_time: 93.17 min
[20231206-084309] [INFO] Epoch: 28, train_loss: 2.7646, val_loss: 3.4065, train_acc: 0.2701, val_acc: 0.2250
[20231206-084309] [INFO] elapsed_time: 96.62 min
[20231206-084635] [INFO] Epoch: 29, train_loss: 2.7342, val_loss: 4.3990, train_acc: 0.2815, val_acc: 0.1626
[20231206-084635] [INFO] elapsed_time: 100.07 min
[20231206-085002] [INFO] Epoch: 30, train_loss: 2.7117, val_loss: 3.7581, train_acc: 0.2817, val_acc: 0.1932
[20231206-085002] [INFO] elapsed_time: 103.51 min
[20231206-085330] [INFO] Epoch: 31, train_loss: 2.6067, val_loss: 3.6970, train_acc: 0.3008, val_acc: 0.2137
[20231206-085330] [INFO] elapsed_time: 106.97 min
[20231206-085657] [INFO] Epoch: 32, train_loss: 2.5870, val_loss: 3.5865, train_acc: 0.3092, val_acc: 0.2273
[20231206-085657] [INFO] elapsed_time: 110.42 min
[20231206-090024] [INFO] Epoch: 33, train_loss: 2.5723, val_loss: 3.5069, train_acc: 0.3062, val_acc: 0.2368
[20231206-090024] [INFO] elapsed_time: 113.88 min
[20231206-090352] [INFO] Epoch: 34, train_loss: 2.5767, val_loss: 3.4438, train_acc: 0.3086, val_acc: 0.2317
[20231206-090352] [INFO] elapsed_time: 117.34 min
[20231206-090719] [INFO] Epoch: 35, train_loss: 2.4225, val_loss: 3.7364, train_acc: 0.3400, val_acc: 0.2219
[20231206-090719] [INFO] elapsed_time: 120.80 min
[20231206-091046] [INFO] Epoch: 36, train_loss: 2.4329, val_loss: 3.7602, train_acc: 0.3425, val_acc: 0.2192
[20231206-091046] [INFO] elapsed_time: 124.25 min
[20231206-091412] [INFO] Epoch: 37, train_loss: 2.3973, val_loss: 5.3163, train_acc: 0.3408, val_acc: 0.1370
[20231206-091412] [INFO] elapsed_time: 127.68 min
[20231206-091739] [INFO] Epoch: 38, train_loss: 2.3801, val_loss: 3.9423, train_acc: 0.3468, val_acc: 0.2198
[20231206-091739] [INFO] elapsed_time: 131.13 min
[20231206-092106] [INFO] Epoch: 39, train_loss: 2.3090, val_loss: 4.4708, train_acc: 0.3763, val_acc: 0.1674
[20231206-092106] [INFO] elapsed_time: 134.57 min
[20231206-092432] [INFO] Epoch: 40, train_loss: 2.2754, val_loss: 4.9117, train_acc: 0.3747, val_acc: 0.1980
[20231206-092432] [INFO] elapsed_time: 138.02 min
[20231206-092759] [INFO] Epoch: 41, train_loss: 2.2211, val_loss: 3.4976, train_acc: 0.3918, val_acc: 0.2881
[20231206-092759] [INFO] elapsed_time: 141.46 min
[20231206-093126] [INFO] Epoch: 42, train_loss: 2.1903, val_loss: 4.1813, train_acc: 0.3913, val_acc: 0.2263
[20231206-093126] [INFO] elapsed_time: 144.91 min
[20231206-093453] [INFO] Epoch: 43, train_loss: 2.2871, val_loss: 3.9491, train_acc: 0.3746, val_acc: 0.2465
[20231206-093453] [INFO] elapsed_time: 148.36 min
[20231206-093820] [INFO] Epoch: 44, train_loss: 2.0923, val_loss: 3.6289, train_acc: 0.4134, val_acc: 0.2986
[20231206-093820] [INFO] elapsed_time: 151.81 min
[20231206-094147] [INFO] Epoch: 45, train_loss: 2.0279, val_loss: 4.0269, train_acc: 0.4257, val_acc: 0.2403
[20231206-094147] [INFO] elapsed_time: 155.26 min
[20231206-094513] [INFO] Epoch: 46, train_loss: 1.9890, val_loss: 3.9994, train_acc: 0.4385, val_acc: 0.2432
[20231206-094513] [INFO] elapsed_time: 158.70 min
[20231206-094840] [INFO] Epoch: 47, train_loss: 1.9932, val_loss: 3.8045, train_acc: 0.4367, val_acc: 0.2697
[20231206-094840] [INFO] elapsed_time: 162.15 min
[20231206-095208] [INFO] Epoch: 48, train_loss: 2.0226, val_loss: 4.0147, train_acc: 0.4417, val_acc: 0.2277
[20231206-095208] [INFO] elapsed_time: 165.61 min
[20231206-095535] [INFO] Epoch: 49, train_loss: 1.9837, val_loss: 4.1157, train_acc: 0.4431, val_acc: 0.2546
[20231206-095535] [INFO] elapsed_time: 169.06 min
[20231206-095903] [INFO] Epoch: 50, train_loss: 1.9175, val_loss: 4.3732, train_acc: 0.4643, val_acc: 0.2683
[20231206-095903] [INFO] elapsed_time: 172.52 min
[20231206-100229] [INFO] Epoch: 51, train_loss: 1.9072, val_loss: 3.9299, train_acc: 0.4606, val_acc: 0.2744
[20231206-100229] [INFO] elapsed_time: 175.97 min
[20231206-100556] [INFO] Epoch: 52, train_loss: 1.8655, val_loss: 3.9549, train_acc: 0.4735, val_acc: 0.2675
[20231206-100556] [INFO] elapsed_time: 179.41 min
[20231206-100922] [INFO] Epoch: 53, train_loss: 1.8008, val_loss: 3.7900, train_acc: 0.4838, val_acc: 0.3020
[20231206-100922] [INFO] elapsed_time: 182.85 min
[20231206-100922] [INFO] Early Stopping with Epoch: 52
[20231206-100923] [INFO] argument
[20231206-100923] [INFO]   note: None
[20231206-100923] [INFO]   seed: 24771
[20231206-100923] [INFO]   dir_result: ResNet50/lr1e-3_b512
[20231206-100923] [INFO]   model_name: ResNet50
[20231206-100923] [INFO]   num_epoch: 100
[20231206-100923] [INFO]   batch_size: 512
[20231206-100923] [INFO]   lr: 0.001
[20231206-100923] [INFO]   patience: 25
[20231206-100923] [INFO]   delta: 0.002
[20231206-100923] [INFO]   lr_min: 1e-05
[20231206-100923] [INFO]   warmup_t: 10
[20231206-100923] [INFO]   warmup_lr_init: 1e-05
[20231206-100923] [INFO] loss
[20231206-100923] [INFO]   training: True
[20231206-100923] [INFO]   reduction: mean
[20231206-100923] [INFO]   ignore_index: -100
[20231206-100923] [INFO]   label_smoothing: 0.0
[20231206-100923] [INFO] optimizer
[20231206-100923] [INFO]   defaults: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}
[20231206-100923] [INFO] scheduler
[20231206-100923] [INFO]   optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    initial_lr: 0.001
    lr: 0.000628101494146603
    maximize: False
    weight_decay: 0
)
[20231206-100923] [INFO]   param_group_field: lr
[20231206-100923] [INFO]   base_values: [0.001]
[20231206-100923] [INFO]   metric: None
[20231206-100923] [INFO]   t_in_epochs: True
[20231206-100923] [INFO]   noise_range_t: None
[20231206-100923] [INFO]   noise_pct: 0.67
[20231206-100923] [INFO]   noise_type: normal
[20231206-100923] [INFO]   noise_std: 1.0
[20231206-100923] [INFO]   noise_seed: 42
[20231206-100923] [INFO]   t_initial: 100
[20231206-100923] [INFO]   lr_min: 1e-05
[20231206-100923] [INFO]   cycle_mul: 1.0
[20231206-100923] [INFO]   cycle_decay: 1.0
[20231206-100923] [INFO]   cycle_limit: 1
[20231206-100923] [INFO]   warmup_t: 10
[20231206-100923] [INFO]   warmup_lr_init: 1e-05
[20231206-100923] [INFO]   warmup_prefix: True
[20231206-100923] [INFO]   k_decay: 1.0
[20231206-100923] [INFO]   warmup_steps: [9.9e-05]
